{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07856d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3==1.17.99 in ./opt/anaconda3/lib/python3.8/site-packages (1.17.99)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in ./opt/anaconda3/lib/python3.8/site-packages (from boto3==1.17.99) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.99 in ./opt/anaconda3/lib/python3.8/site-packages (from boto3==1.17.99) (1.20.112)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in ./opt/anaconda3/lib/python3.8/site-packages (from boto3==1.17.99) (0.4.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./opt/anaconda3/lib/python3.8/site-packages (from botocore<1.21.0,>=1.20.99->boto3==1.17.99) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in ./opt/anaconda3/lib/python3.8/site-packages (from botocore<1.21.0,>=1.20.99->boto3==1.17.99) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in ./opt/anaconda3/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.99->boto3==1.17.99) (1.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting pandas==1.3.0\n",
      "  Downloading pandas-1.3.0-cp38-cp38-macosx_10_9_x86_64.whl (11.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in ./opt/anaconda3/lib/python3.8/site-packages (from pandas==1.3.0) (1.22.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in ./opt/anaconda3/lib/python3.8/site-packages (from pandas==1.3.0) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./opt/anaconda3/lib/python3.8/site-packages (from pandas==1.3.0) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in ./opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas==1.3.0) (1.15.0)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.1\n",
      "    Uninstalling pandas-1.5.1:\n",
      "      Successfully uninstalled pandas-1.5.1\n",
      "Successfully installed pandas-1.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: sqlalchemy==1.4.16 in ./opt/anaconda3/lib/python3.8/site-packages (1.4.16)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./opt/anaconda3/lib/python3.8/site-packages (from sqlalchemy==1.4.16) (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: psycopg2-binary in ./opt/anaconda3/lib/python3.8/site-packages (2.9.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3==1.17.99\n",
    "!pip install pandas==1.3.0\n",
    "!pip install sqlalchemy==1.4.16\n",
    "!pip install psycopg2-binary --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3630762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to aws\n",
      "-----------------\n",
      "Renamed dataframes accordingly\n",
      "-----------------\n",
      "Renamed dataframes accordingly\n",
      "-----------------\n",
      "Renamed dataframes accordingly\n",
      "-----------------\n",
      "Merged all files\n",
      "-----------------\n",
      "Replace 1 to win and 0 to loss\n",
      "-----------------\n",
      "List of table names\n",
      "hussein_hamdan\n",
      "jason_aydar\n",
      "jared_wofford\n",
      "jacqueline_ramos\n",
      "ryan_clark\n",
      "christopher_carter\n",
      "stephen_bosch\n",
      "anthony_barone\n",
      "ryan_held\n",
      "john_hill\n",
      "matthew_fioravanti\n",
      "johnny_williams\n",
      "michael_leib\n",
      "chad_meyer\n",
      "brett_segraves\n",
      "shakti_jagadish\n",
      "dean_chirieleison\n",
      "Anthony_Barone\n",
      "joe_seroski\n",
      "-----------------\n",
      "Any columns or data in the table deleted if exists\n",
      "-----------------\n",
      "Check if column name exists\n",
      "-----------------\n",
      "Add columns to table\n",
      "-----------------\n",
      "Check if column name exists\n",
      "week\n",
      "opponent\n",
      "location\n",
      "result\n",
      "boyd_yards\n",
      "boyd_td\n",
      "chase_yards\n",
      "chase_td\n",
      "higgins_yards\n",
      "higgins_td\n",
      "-----------------\n",
      "Data load started\n",
      "-----------------\n",
      "Check for loaded data\n",
      "('PRE1', 'TB', 'Away', 'Win', '', '', '', '', '', '')\n",
      "('PRE2', 'WSH', 'Away', 'Loss', '', '', '', '', '', '')\n",
      "('PRE3', 'MIA', 'Home', 'Loss', '', '', '', '', '', '')\n",
      "('REG1', 'MIN', 'Home', 'Win', '32.0', '0.0', '101.0', '1.0', '58.0', '1.0')\n",
      "('REG2', 'CHI', 'Away', 'Loss', '73.0', '0.0', '54.0', '1.0', '60.0', '1.0')\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from io import BytesIO\n",
    "from io import StringIO\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, access_key, secret_key, database, user, password, host, port):\n",
    "        self.access_key = access_key\n",
    "        self.secret_key = secret_key\n",
    "        self.database = database\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        \n",
    "    # Establish connection with AWS S3\n",
    "    def connect_to_s3(self, access_key, secret_key):\n",
    "        print('Connecting to aws')\n",
    "        print('-----------------')\n",
    "        return boto3.client('s3',\n",
    "                            aws_access_key_id=access_key,\n",
    "                            aws_secret_access_key=secret_key)\n",
    "    \n",
    "    # Rename columns in DataFrame   \n",
    "    def rename_dataframe_columns(self, dataframe, rename_dict):\n",
    "        print('Renamed dataframes accordingly')\n",
    "        print('-----------------')\n",
    "        return dataframe.rename(columns=rename_dict)\n",
    "    \n",
    "    # Merge multiple DataFrames\n",
    "    def merge_dataframes(self, dfs, merge_key='Week', merge_how='left'):\n",
    "        merged_df = dfs[0]\n",
    "        for df in dfs[1:]:\n",
    "            merged_df = pd.merge(merged_df, df, on=merge_key, how=merge_how)\n",
    "        print('Merged all files')\n",
    "        print('-----------------')\n",
    "        return merged_df\n",
    "    \n",
    "    # Replace values in a DataFrame\n",
    "    def replace_dataframe_values(self, dataframe, replacement_dict):\n",
    "        print(\"Replace 1 to win and 0 to loss\")\n",
    "        print('-----------------')\n",
    "        return dataframe.replace(replacement_dict)\n",
    "    \n",
    "    # Establish connection with PostgreSQL database\n",
    "    def connect_to_postgres(self):\n",
    "        conn = psycopg2.connect(\n",
    "            database=self.database,\n",
    "            user=self.user,\n",
    "            password=self.password,\n",
    "            host=self.host,\n",
    "            port=self.port\n",
    "        )\n",
    "        return conn\n",
    "    \n",
    "    # Display list of tables in the database\n",
    "    def display_table_names(self):\n",
    "        # Create connection to PostgreSQL database\n",
    "        print('List of table names')\n",
    "        conn = self.connect_to_postgres()\n",
    "        cursor = conn.cursor()\n",
    "        # Fetch table names from the public schema\n",
    "        cursor.execute(\"SELECT tablename FROM pg_catalog.pg_tables WHERE schemaname = 'public'\")\n",
    "        table_names = cursor.fetchall()\n",
    "        for table in table_names:\n",
    "            print(table[0])\n",
    "        # Close the cursor and database connection\n",
    "        print('-----------------')\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "     # Delete column names and data from table if it exists\n",
    "    def delete_columns_and_data(self):\n",
    "        conn = self.connect_to_postgres()\n",
    "        cur = conn.cursor()\n",
    "        print('Any columns or data in the table deleted if exists')\n",
    "        cur.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'shakti_jagadish'\")\n",
    "        columns = cur.fetchall()\n",
    "        for column in columns:\n",
    "            cur.execute(f\"ALTER TABLE shakti_jagadish DROP COLUMN {column[0]}\")\n",
    "        cur.execute(\"DELETE FROM shakti_jagadish\")\n",
    "        conn.commit()\n",
    "        print('-----------------')\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "    # Check if column names exist in the database befor and after adding columns\n",
    "    def check_column_names(self):\n",
    "        print('Check if column name exists')\n",
    "        conn = self.connect_to_postgres()\n",
    "        cur = conn.cursor()\n",
    "        query = f\"SELECT column_name FROM information_schema.columns WHERE table_schema = 'public' AND table_name = 'shakti_jagadish'\"\n",
    "        cur.execute(query)\n",
    "        column_names = [row[0] for row in cur.fetchall()]\n",
    "        for column in column_names:\n",
    "            print(column)           \n",
    "        print('-----------------')\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    \n",
    "    #Add columns to the table\n",
    "    def add_columns_to_table(self):\n",
    "        # Create connection to PostgreSQL database\n",
    "        conn = self.connect_to_postgres()\n",
    "        cur = conn.cursor()\n",
    "        print('Add columns to table')\n",
    "        # Get the column names from the DataFrame\n",
    "        column_names = merged_df.columns.tolist()\n",
    "        # Generate the ALTER TABLE statements to add columns\n",
    "        alter_table_statements = [f\"ALTER TABLE shakti_jagadish ADD COLUMN {column_name} VARCHAR(255);\" for column_name in column_names]\n",
    "        # Execute the ALTER TABLE statements\n",
    "        for statement in alter_table_statements:\n",
    "            cur.execute(statement)\n",
    "        # Commit the changes\n",
    "        conn.commit()\n",
    "        print('-----------------')\n",
    "        # Close the cursor and connection\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    \n",
    "    #Load the data into table\n",
    "    def load_data_to_table(self):\n",
    "        \n",
    "        print(\"Data load started\")    \n",
    "        # Create connection to PostgreSQL database\n",
    "        conn = self.connect_to_postgres()\n",
    "        cur = conn.cursor()     \n",
    "        # Create a buffer to hold the DataFrame data\n",
    "        buffer = StringIO()\n",
    "        merged_df.to_csv(buffer, index=False, header=False, sep='\\t')\n",
    "        # Reset the buffer position to the start\n",
    "        buffer.seek(0)   \n",
    "        cur.copy_from(buffer,'shakti_jagadish', sep='\\t')\n",
    "        # Commit the changes\n",
    "        conn.commit()\n",
    "        print('-----------------')\n",
    "        # Close the cursor and connection\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    \n",
    "    #Check for the loaded data in table\n",
    "    def check_loaded_data(self):\n",
    "        \n",
    "        print(\"Check for loaded data\")       \n",
    "        # Create connection to PostgreSQL database\n",
    "        conn = self.connect_to_postgres()\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(f\"SELECT * FROM shakti_jagadish LIMIT 5\")\n",
    "        # Fetch the records\n",
    "        records = cur.fetchall()\n",
    "        # Print the fetched records\n",
    "        for row in records:\n",
    "            print(row)           \n",
    "        print('-----------------')\n",
    "        # Close the cursor and connection\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    data_processor = DataProcessor(\n",
    "    access_key='AKIAZZ33YB65GZIN656A',\n",
    "    secret_key='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',\n",
    "    database='postgres',\n",
    "    user='shakti_jagadish',\n",
    "    password='jhaktisagadish',\n",
    "    host='ls-2619b6b15c9bdc80a23f6afb7eee54cf0247da21.ca3yee6xneaj.us-east-1.rds.amazonaws.com',\n",
    "    port='5432')\n",
    "    # Call the connect_to_s3 function\n",
    "    s3_client = data_processor.connect_to_s3(access_key=data_processor.access_key, secret_key=data_processor.secret_key)\n",
    "    # Download files from S3 bucket and read them into DataFrames\n",
    "    bucket_name = 'mindex-data-analytics-code-challenge'\n",
    "    file_keys = ['bengals.csv', 'boyd_receiving.csv', 'chase_receiving.csv', 'higgins_receiving.csv']\n",
    "    dfs = []  # List to store the DataFrames\n",
    "    for file_key in file_keys:\n",
    "        obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "        df = pd.read_csv(io.BytesIO(obj['Body'].read()), encoding='utf8')\n",
    "        dfs.append(df)\n",
    "    df, df1, df2, df3 = dfs  # Assign each DataFrame to respective variables\n",
    "    # Rename columns in df1, df2, and df3\n",
    "    rename_dict1 = {'Yards': 'Boyd_Yards', 'TD': 'Boyd_TD'}\n",
    "    rename_dict2 = {'Yards': 'Chase_Yards', 'TD': 'Chase_TD'}\n",
    "    rename_dict3 = {'Yards': 'Higgins_Yards', 'TD': 'Higgins_TD'}\n",
    "    df1 = data_processor.rename_dataframe_columns(df1, rename_dict1)\n",
    "    df2 = data_processor.rename_dataframe_columns(df2, rename_dict2)\n",
    "    df3 = data_processor.rename_dataframe_columns(df3, rename_dict3)\n",
    "    # Merge the downloaded DataFrames\n",
    "    merged_df = data_processor.merge_dataframes([df, df1, df2, df3])\n",
    "    # Replace values in the DataFrame\n",
    "    replacement_dict = {'Result': {1.0: 'Win', 0.0: 'Loss'}}\n",
    "    merged_df = data_processor.replace_dataframe_values(merged_df, replacement_dict)\n",
    "    # Display table names\n",
    "    data_processor.display_table_names()\n",
    "    #Delete column names and data inside the table if exists\n",
    "    data_processor.delete_columns_and_data()\n",
    "    # Call the check_column_names function\n",
    "    data_processor.check_column_names()\n",
    "    # Add column names to the table\n",
    "    data_processor.add_columns_to_table()\n",
    "    # Call the check_column_names function to check if columns are loaded\n",
    "    data_processor.check_column_names()\n",
    "    # Load the data to tablle\n",
    "    data_processor.load_data_to_table()\n",
    "    # Call the check_loaded_data function to check if columns are loaded\n",
    "    data_processor.check_loaded_data()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e685e279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a943ec98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2628b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd77dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e0e52c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc986e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6e7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51130341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77bd813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d964e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2319468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236b5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59433dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c25aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846cac35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a65821f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246c6c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab1ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ddb34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f76f6af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
